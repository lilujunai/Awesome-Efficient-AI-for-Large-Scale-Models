# Awesome-Efficient-AI-for-Large-Scale-Models
Paper survey of efficient computation for large scale models.

##  Knowledge Distillation in Vision Transformers


Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). Training data-efficient image transformers &
distillation through attention. In International Conference on Machine Learning (pp. 10347-10357). PMLR

Ren, S., Gao, Z., Hua, T., Xue, Z., Tian, Y., He, S., & Zhao, H. (2022). Co-advise: Cross inductive bias distillation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16773-16782).

Wu, K., Zhang, J., Peng, H., Liu, M., Xiao, B., Fu, J., & Yuan, L. (2022). Tinyvit: Fast pretraining distillation for small vision
transformers. arXiv preprint arXiv:2207.10666.

Wang, J., Cao, M., Shi, S., Wu, B., & Yang, Y. (2022, May). Attention Probe: Vision Transformer Distillation in the Wild. ICASSP
2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2220-2224). IEEE.

Xiang, C., Qiong, C., Yujie, Z., Jing, Z., Shenghua, G., & Dacheng, T. (2022). Dear KD: Data-Efficient Early Knowledge Distillation
for Vision Transformers. arXiv preprint arXiv:2204.12997


Jia, D., Han, K., Wang, Y., Tang, Y., Guo, J., Zhang, C., & Tao, D. (2021). Efficient vision transformers via fine-grained manifold
distillation. arXiv preprint arXiv:2107.01378.

Wu, K., Zhang, J., Peng, H., Liu, M., Xiao, B., Fu, J., & Yuan, L. (2022). TinyViT: Fast Pretraining Distillation for Small Vision
Transformers. arXiv preprint arXiv:2207.10666.

Liu, Y., Cao, J., Li, B., Hu, W., Ding, J., & Li, L. (2022). Cross-Architecture Knowledge Distillation. arXiv preprint arXiv:2207.05273.


Zhang, J., Peng, H., Wu, K., Liu, M., Xiao, B., Fu, J., & Yuan, L. (2022). MiniViT: Compressing Vision Transformers with Weight
Multiplexing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12145-12154).

ViTKD: Practical Guidelines for ViT feature knowledge distillation, Zhendong Yang, Zhe Li, Ailing Zeng, Zexian Li, Chun Yuan, Yu Li, arXiv 2022, code

On Distillation of Guided Diffusion Models, Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, Tim Salimans, arXiv 2022

##   Pruning for Vision Transformers

Yu, S., Chen, T., Shen, J., Yuan, H., Tan, J., Yang, S., ... & Wang, Z. (2022). Unified visual transformer compression. arXiv preprint
arXiv:2203.08243.

Self-slimmed Vision Transformer, Zhuofan Zong, Kunchang Li, Guanglu Song, Yali Wang, Yu Qiao, Biao Leng, Yu Liu, ICLR 2022



##   Quantization for Vision Transformers

[arxiv] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models.

[NeurIPS] Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer. [qnn]
 
 [ECCV] PTQ4ViT: Post-Training Quantization for Vision Transformers with Twin Uniform Quantization. [qnn]
  
  [ECCV] Patch Similarity Aware Data-Free Quantization for Vision Transformers. [qnn]
  
  [IJCAI] FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer. [qnn] [code] [71⭐]
    
   [arxiv] Q-ViT: Fully Differentiable Quantization for Vision Transformer [qnn]
   
   [NeurIPS] BiT: Robustly Binarized Multi-distilled Transformer. [bnn] [code] [42⭐]
  
  [NeurIPS] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [qnn]




##  Methods for Distillation Gaps

Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher. Mirzadeh et al. arXiv:1902.03393

Search to Distill: Pearls are Everywhere but not the Eyes. Liu Yu et al. CVPR 2020

Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation, Jia Guo, Minghao Chen, Yao Hu, Chen Zhu, Xiaofei He, Deng Cai, 2020

Decoupled Knowledge Distillation, Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, Jiajun Liang, CVPR 2022, code

Prune Your Model Before Distill It, Jinhyuk Park and Albert No, ECCV 2022, code



Huang, T., You, S., Wang, F., Qian, C., & Xu, C. (2022). Knowledge Distillation from A Stronger Teacher. arXiv preprint
arXiv:2205.10536.

Masked Generative Distillation, Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, Chun Yuan, ECCV 2022, code


Curriculum Temperature for Knowledge Distillation, Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, Jian Yang, AAAI 2023, code

Knowledge distillation: A good teacher is patient and consistent, Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, Alexander Kolesnikov, CVPR 2022

Knowledge Distillation with the Reused Teacher Classifier, Defang Chen, Jian-Ping Mei, Hailin Zhang, Can Wang, Yan Feng, Chun Chen, CVPR 2022




